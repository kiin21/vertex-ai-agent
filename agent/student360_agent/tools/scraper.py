# -------- Core Tools --------

import requests
from bs4 import BeautifulSoup
from urllib.parse import quote_plus
import time
import random
import re


def google_search_jobs(query: str, location: str = "", max_results: int = 10) -> list[dict]:
    """
    Use Google Custom Search API to find job postings from Vietnamese job sites

    Args:
        query: Job search terms (e.g., "backend developer java")
        location: Location preference (e.g., "TP.HCM", "HÃ  Ná»™i")
        max_results: Maximum number of results to return

    Returns:
        list of job dictionaries with basic info from Google search
    """
    import os

    # Get API credentials
    api_key = os.getenv('GOOGLE_API_KEY')
    search_engine_id = os.getenv('GOOGLE_CSE_ID')

    if not api_key or not search_engine_id:
        print("Google API credentials not found, using fallback scraping")
        return []

    # Target Vietnamese job sites
    job_sites = [
        "site:topcv.vn",
        "site:vietnamworks.com",
        "site:topdev.vn",
        "site:itviec.com",
        "site:careerbuilder.vn"
    ]

    # Build comprehensive search query
    site_filter = " OR ".join(job_sites)
    full_query = f"{query}"
    if location:
        full_query += f" {location}"
    full_query += f" ({site_filter})"

    # Add Vietnamese equivalent terms
    vietnamese_terms = {
        'developer': 'láº­p trÃ¬nh viÃªn',
        'engineer': 'ká»¹ sÆ° pháº§n má»m',
        'internship': 'thá»±c táº­p sinh',
        'manager': 'trÆ°á»Ÿng nhÃ³m',
        'backend': 'backend',
        'frontend': 'frontend',
        'fullstack': 'full-stack'
    }

    # Add Vietnamese alternatives
    vn_alternatives = []
    for en_term, vn_term in vietnamese_terms.items():
        if en_term in query.lower():
            vn_alternatives.append(vn_term)

    if vn_alternatives:
        full_query += f" OR ({' '.join(vn_alternatives)})"

    try:
        # Google Custom Search API call
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            'key': api_key,
            'cx': search_engine_id,
            'q': full_query,
            'num': min(max_results, 10),  # API limit per request
            'lr': 'lang_vi',  # Vietnamese language
            'gl': 'vn',       # Vietnam country
            'safe': 'active'
        }

        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()

        jobs = []
        for item in data.get('items', []):
            # Extract job info from Google results
            title = item.get('title', '')
            snippet = item.get('snippet', '')
            url = item.get('link', '')

            # Parse company and other details from snippet and title
            company = extract_company_from_google_result(title, snippet)
            salary = extract_salary_from_snippet(snippet)
            job_location = location if location else extract_location_from_snippet(
                snippet)
            source = extract_source_from_url(url)

            jobs.append({
                'title': clean_job_title(title),
                'company': company,
                'location': job_location,
                'salary': salary,
                'url': url,
                'source': source,
                'snippet': snippet[:200] + "..." if len(snippet) > 200 else snippet
            })

        return jobs

    except requests.exceptions.RequestException as e:
        print(f"Google API request error: {e}")
        return []
    except Exception as e:
        print(f"Google search error: {e}")
        return []


def web_scrape_jobs(query: str, location: str = "", pages: int = 1) -> list[dict]:
    """
    Enhanced web scraping for job sites with better reliability

    Args:
        query: Search query
        location: Location filter
        pages: Number of pages to scrape

    Returns:
        list of detailed job dictionaries
    """

    def scrape_topcv(query: str, location: str, page: int) -> list[dict]:
        """Scrape TopCV with improved selectors"""
        try:
            url = f"https://www.topcv.vn/viec-lam?q={quote_plus(query)}"
            if location:
                url += f"&l={quote_plus(location)}"
            url += f"&page={page}"

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "vi-VN,vi;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "Cache-Control": "no-cache"
            }

            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            jobs = []

            # Multiple selector strategies for robustness
            job_selectors = [
                "[data-cy='job-card']",
                ".job-item",
                ".job-list-search-result .job-item",
                ".search-result .job-item"
            ]

            for selector in job_selectors:
                cards = soup.select(selector)
                if cards:
                    break

            for card in cards:
                # Try multiple title selectors
                title_el = None
                title_selectors = ["a[href*='/viec-lam/']",
                                   ".title a", "h3 a", ".job-title a"]
                for sel in title_selectors:
                    title_el = card.select_one(sel)
                    if title_el:
                        break

                if not title_el:
                    continue

                # Extract other info with fallbacks
                company_el = card.select_one(
                    ".company, .job-company, [data-cy='company-name'], .company-name")
                loc_el = card.select_one(
                    ".address, .location, [data-cy='job-location'], .job-location")
                salary_el = card.select_one(
                    ".salary, [data-cy='job-salary'], .job-salary")

                url = title_el.get("href", "")
                if url and url.startswith("/"):
                    url = "https://www.topcv.vn" + url

                jobs.append({
                    'title': title_el.get_text(strip=True),
                    'company': company_el.get_text(strip=True) if company_el else "",
                    'location': loc_el.get_text(strip=True) if loc_el else location,
                    'salary': salary_el.get_text(strip=True) if salary_el else "Thá»a thuáº­n",
                    'url': url,
                    'source': 'topcv'
                })

            return jobs

        except Exception as e:
            print(f"Error scraping TopCV: {e}")
            return []

    def scrape_vietnamworks(query: str, location: str, page: int) -> list[dict]:
        """Scrape VietnamWorks with enhanced error handling"""
        try:
            # Similar implementation with multiple selector fallbacks
            # and better error handling
            pass
        except Exception as e:
            print(f"Error scraping VietnamWorks: {e}")
            return []

    def scrape_topdev(query: str, location: str, page: int) -> list[dict]:
        """Scrape TopDev for tech jobs"""
        try:
            # Implementation for TopDev
            pass
        except Exception as e:
            print(f"Error scraping TopDev: {e}")
            return []

    # Execute scraping across sites
    all_jobs = []
    scrapers = [scrape_topcv, scrape_vietnamworks, scrape_topdev]

    for scraper in scrapers:
        for page in range(1, pages + 1):
            try:
                jobs = scraper(query, location, page)
                all_jobs.extend(jobs)
                time.sleep(random.uniform(1, 2))  # Rate limiting
            except Exception as e:
                print(f"Scraper error: {e}")
                continue

    # Remove duplicates
    seen_urls = set()
    unique_jobs = []
    for job in all_jobs:
        if job['url'] not in seen_urls:
            seen_urls.add(job['url'])
            unique_jobs.append(job)

    return unique_jobs


def optimize_search_query(user_request: str, user_profile: dict) -> list[str]:
    """
    Generate optimized search queries from user input

    Args:
        user_request: Original user request
        user_profile: User profile with skills, experience, etc.

    Returns:
        list of optimized search queries
    """
    base_queries = []

    # Extract key terms from user request
    skills = user_profile.get('skills', [])
    experience_level = user_profile.get('experience_years', 0)

    # Generate base query variations
    if 'backend' in user_request.lower():
        base_queries.extend([
            "backend developer",
            "backend engineer",
            "server side developer"
        ])

    if 'frontend' in user_request.lower():
        base_queries.extend([
            "frontend developer",
            "frontend engineer",
            "web developer"
        ])

    # Add skill-specific queries
    for skill in skills[:3]:  # Top 3 skills
        base_queries.append(f"{skill} developer")
        if experience_level <= 1:
            base_queries.append(f"{skill} junior")
            base_queries.append(f"{skill} intern")
        elif experience_level <= 3:
            base_queries.append(f"{skill} junior developer")
        else:
            base_queries.append(f"senior {skill}")

    # Add experience level qualifiers
    if experience_level <= 1:
        base_queries.extend([
            query + " junior" for query in base_queries[:3]
        ])
        base_queries.extend([
            query + " fresher" for query in base_queries[:3]
        ])

    # Remove duplicates and limit
    unique_queries = list(dict.fromkeys(base_queries))
    return unique_queries[:5]  # Top 5 queries


def merge_and_deduplicate_jobs(google_jobs: list[dict], scraped_jobs: list[dict]) -> list[dict]:
    """
    Merge results from Google search and scraping, remove duplicates

    Args:
        google_jobs: Jobs found via Google search
        scraped_jobs: Jobs found via web scraping

    Returns:
        Merged and deduplicated job list
    """
    all_jobs = []
    seen_urls = set()
    seen_titles_companies = set()

    # Add all jobs while checking for duplicates
    for job_list in [google_jobs, scraped_jobs]:
        for job in job_list:
            url = job.get('url', '')
            title = job.get('title', '').lower().strip()
            company = job.get('company', '').lower().strip()

            # Skip if duplicate URL
            if url and url in seen_urls:
                continue

            # Skip if same title + company (likely duplicate)
            title_company_key = f"{title}|{company}"
            if title_company_key in seen_titles_companies:
                continue

            # Add to results
            if url:
                seen_urls.add(url)
            seen_titles_companies.add(title_company_key)
            all_jobs.append(job)

    return all_jobs


def analyze_and_score_jobs(jobs: list[dict], user_profile: dict) -> list[dict]:
    """
    Analyze and score jobs based on user profile

    Args:
        jobs: list of job dictionaries
        user_profile: User profile with preferences

    Returns:
        list of jobs with scores and reasoning
    """
    scored_jobs = []

    user_skills = [skill.lower() for skill in user_profile.get('skills', [])]
    user_location = user_profile.get('location', '').lower()
    user_experience = user_profile.get('experience_years', 0)
    expected_salary = user_profile.get('expected_salary', 0)

    for job in jobs:
        score = 0
        reasons = []

        title = job.get('title', '').lower()
        company = job.get('company', '').lower()
        location = job.get('location', '').lower()
        salary = job.get('salary', '').lower()

        # Skill matching (highest weight)
        skill_matches = 0
        for skill in user_skills:
            if skill in title or skill in job.get('snippet', '').lower():
                score += 5
                skill_matches += 1
                reasons.append(f"Khá»›p skill {skill.title()}")

        # Bonus for multiple skill matches
        if skill_matches >= 2:
            score += 3
            reasons.append("Khá»›p nhiá»u skills quan trá»ng")

        # Experience level matching
        if user_experience <= 1:
            if any(term in title for term in ['junior', 'fresher', 'intern', 'trainee']):
                score += 4
                reasons.append("PhÃ¹ há»£p level fresher/junior")
        elif 1 < user_experience <= 3:
            if 'junior' in title and 'senior' not in title:
                score += 4
                reasons.append("PhÃ¹ há»£p level junior")
        else:
            if any(term in title for term in ['senior', 'lead', 'principal']):
                score += 4
                reasons.append("PhÃ¹ há»£p level senior")

        # Location matching
        if user_location and user_location in location:
            score += 3
            reasons.append("ÄÃºng khu vá»±c mong muá»‘n")

        # Remote work bonus
        if any(term in (title + location) for term in ['remote', 'work from home', 'wfh']):
            score += 2
            reasons.append("Há»— trá»£ lÃ m viá»‡c remote")

        # Salary analysis (basic)
        if expected_salary > 0 and salary and salary != 'thá»a thuáº­n':
            # Simple salary extraction
            salary_numbers = [int(x) for x in salary.split() if x.isdigit()]
            # Convert to millions
            if salary_numbers and max(salary_numbers) >= expected_salary/1000000:
                score += 2
                reasons.append("ÄÃ¡p á»©ng má»©c lÆ°Æ¡ng mong Ä‘á»£i")

        # Company reputation (basic check)
        reputable_keywords = ['tech', 'technology',
                              'software', 'digital', 'innovation']
        if any(keyword in company for keyword in reputable_keywords):
            score += 1
            reasons.append("CÃ´ng ty cÃ´ng nghá»‡ uy tÃ­n")

        scored_jobs.append({
            'job': job,
            'score': score,
            'reasons': reasons,
            'match_percentage': min(100, (score / 25) * 100)
        })

    # Sort by score
    scored_jobs.sort(key=lambda x: x['score'], reverse=True)
    return scored_jobs


def format_job_results(scored_jobs: list[dict], search_summary: dict) -> str:
    """
    Format final job recommendations in Vietnamese-friendly markdown

    Args:
        scored_jobs: Jobs with scores and reasons
        search_summary: Summary of search process

    Returns:
        Formatted markdown string
    """
    if not scored_jobs:
        return "# âŒ KhÃ´ng tÃ¬m tháº¥y viá»‡c lÃ m phÃ¹ há»£p\n\nVui lÃ²ng thá»­ vá»›i tá»« khÃ³a khÃ¡c hoáº·c má»Ÿ rá»™ng khu vá»±c tÃ¬m kiáº¿m."

    result = "# ğŸ¯ Káº¿t quáº£ TÃ¬m kiáº¿m Viá»‡c lÃ m\n\n"

    # Search summary
    result += "## ğŸ“Š Tá»•ng quan\n"
    result += f"- **Tá»•ng sá»‘ viá»‡c lÃ m:** {search_summary.get('total_found', 0) or 0}\n"
    sources_used = search_summary.get('sources_used', []) or []
    result += f"- **Nguá»“n tÃ¬m kiáº¿m:** {', '.join(sources_used) if sources_used else 'N/A'}\n"
    result += f"- **Tá»« khÃ³a:** {search_summary.get('queries_used') or 'N/A'}\n"
    result += f"- **Khu vá»±c:** {search_summary.get('location') or 'ToÃ n quá»‘c'}\n\n"

    # Top recommendations
    result += "## ğŸ† Top Gá»£i Ã½ PhÃ¹ há»£p Nháº¥t\n\n"

    top_jobs = scored_jobs[:5]  # Top 5

    for i, job_data in enumerate(top_jobs, 1):
        job = job_data['job']
        score = job_data['score']
        reasons = job_data['reasons']
        match_pct = job_data['match_percentage']

        # Job header with match percentage
        job_title = job.get('title') or 'N/A'
        result += f"### {i}. {job_title} "

        # Match indicator
        if match_pct >= 80:
            result += "ğŸ”¥ **KHá»šP HOÃ€N Háº¢O**"
        elif match_pct >= 60:
            result += "â­ **PHá»ª Há»¢P**"
        elif match_pct >= 40:
            result += "âœ… **CÃ“ THá»‚ XEM XÃ‰T**"
        else:
            result += "ğŸ“ **THAM KHáº¢O**"

        result += f" ({match_pct:.0f}%)\n\n"

        # Job details table
        result += "| ThÃ´ng tin | Chi tiáº¿t |\n"
        result += "|-----------|----------|\n"
        result += f"| ğŸ¢ **CÃ´ng ty** | {job.get('company', 'N/A') or 'N/A'} |\n"
        result += f"| ğŸ“ **Äá»‹a Ä‘iá»ƒm** | {job.get('location', 'N/A') or 'N/A'} |\n"
        result += f"| ğŸ’° **Má»©c lÆ°Æ¡ng** | {job.get('salary', 'Thá»a thuáº­n') or 'Thá»a thuáº­n'} |\n"
        result += f"| ğŸŒ **Nguá»“n** | {(job.get('source') or 'N/A').title()} |\n\n"

        # Why it matches
        if reasons:
            result += "**ğŸ¯ LÃ½ do phÃ¹ há»£p:**\n"
            for reason in reasons:
                result += f"- {reason}\n"
            result += "\n"

        # Action button
        job_url = job.get('url') or '#'
        result += f"**ğŸ‘‰ [Xem chi tiáº¿t & á»¨ng tuyá»ƒn]({job_url})**\n\n"
        result += "---\n\n"

    # Additional suggestions if low scores
    if top_jobs and top_jobs[0]['match_percentage'] < 60:
        result += "## ğŸ’¡ Gá»£i Ã½ Cáº£i thiá»‡n TÃ¬m kiáº¿m\n\n"
        result += "- Thá»­ má»Ÿ rá»™ng khu vá»±c tÃ¬m kiáº¿m\n"
        result += "- Xem xÃ©t cÃ¡c vá»‹ trÃ­ junior/trainee náº¿u báº¡n má»›i báº¯t Ä‘áº§u\n"
        result += "- Cáº­p nháº­t thÃªm skills trong profile\n"
        result += "- Thá»­ cÃ¡c tá»« khÃ³a tÆ°Æ¡ng tá»± (VD: 'láº­p trÃ¬nh viÃªn' thay vÃ¬ 'developer')\n\n"

    # General application tips
    result += "## ğŸš€ Tips á»¨ng tuyá»ƒn ThÃ nh cÃ´ng\n\n"
    result += "1. **TÃ¹y chá»‰nh CV** cho tá»«ng vá»‹ trÃ­ - highlight skills phÃ¹ há»£p\n"
    result += "2. **Research cÃ´ng ty** trÆ°á»›c khi apply\n"
    result += "3. **Viáº¿t cover letter** ngáº¯n gá»n, táº­p trung vÃ o value báº¡n mang láº¡i\n"
    result += "4. **Follow up** sau 3-5 ngÃ y náº¿u chÆ°a cÃ³ pháº£n há»“i\n"
    result += "5. **Chuáº©n bá»‹ cÃ¢u há»i** Ä‘á»ƒ há»i interviewer\n\n"

    return result


def extract_user_requirements(user_input: str) -> dict:
    """
    Extract job requirements from natural language input

    Args:
        user_input: User's job search request

    Returns:
        Structured requirements dictionary
    """
    requirements = {
        'query_terms': [],
        'skills': [],
        'experience_keywords': [],
        'location_hints': [],
        'job_type': 'full-time'
    }

    # Common Vietnamese location names
    vn_locations = {
        'há»“ chÃ­ minh': 'TP.HCM', 'ho chi minh': 'TP.HCM', 'saigon': 'TP.HCM',
        'hÃ  ná»™i': 'HÃ  Ná»™i', 'hanoi': 'HÃ  Ná»™i',
        'Ä‘Ã  náºµng': 'ÄÃ  Náºµng', 'da nang': 'ÄÃ  Náºµng'
    }

    # Extract locations
    for location_key, standard_name in vn_locations.items():
        if location_key in user_input.lower():
            requirements['location_hints'].append(standard_name)

    # Extract experience level
    if any(term in user_input.lower() for term in ['intern', 'thá»±c táº­p', 'fresher']):
        requirements['experience_keywords'].append('intern')
    elif any(term in user_input.lower() for term in ['junior', 'má»›i ra trÆ°á»ng']):
        requirements['experience_keywords'].append('junior')
    elif any(term in user_input.lower() for term in ['senior', 'kinh nghiá»‡m']):
        requirements['experience_keywords'].append('senior')

    # Extract job type
    if any(term in user_input.lower() for term in ['part-time', 'bÃ¡n thá»i gian']):
        requirements['job_type'] = 'part-time'
    elif any(term in user_input.lower() for term in ['internship', 'thá»±c táº­p']):
        requirements['job_type'] = 'internship'

    # Extract technical skills
    tech_skills = ['java', 'python', 'javascript', 'react', 'vue', 'angular',
                   'node.js', 'spring', 'spring boot', 'mysql', 'postgresql']

    for skill in tech_skills:
        if skill in user_input.lower():
            requirements['skills'].append(skill)

    return requirements

# -------- Helper Functions for Google Search --------


def extract_company_from_google_result(title: str, snippet: str) -> str:
    """Extract company name from Google search result"""
    # Look for company indicators in title
    title_parts = title.split(' - ')
    if len(title_parts) >= 2:
        # Usually format: "Job Title - Company Name"
        return title_parts[-1].strip()

    # Look in snippet for company indicators
    company_indicators = ['cÃ´ng ty', 'company', 'táº¡i ', 'lÃ m viá»‡c táº¡i']
    for indicator in company_indicators:
        if indicator in snippet.lower():
            # Extract text after indicator
            parts = snippet.lower().split(indicator)
            if len(parts) > 1:
                potential_company = parts[1].split(
                    '.')[0].split(',')[0].strip()
                if len(potential_company) < 50:  # Reasonable company name length
                    return potential_company.title()

    return "N/A"


def extract_salary_from_snippet(snippet: str) -> str:
    """Extract salary from snippet text"""
    import re

    # Vietnamese salary patterns
    salary_patterns = [
        r'(\d+)-(\d+)\s*(triá»‡u|tr|million)',  # 15-25 triá»‡u
        r'(\d+)\s*(triá»‡u|tr|million)',         # 20 triá»‡u
        r'(\d+,?\d*)\s*-\s*(\d+,?\d*)\s*VND',  # 15,000,000 - 25,000,000 VND
        r'lÆ°Æ¡ng:\s*([^.]+)',                   # lÆ°Æ¡ng: 15-25 triá»‡u
        r'salary:\s*([^.]+)'                   # salary: $1000-2000
    ]

    for pattern in salary_patterns:
        match = re.search(pattern, snippet.lower())
        if match:
            return match.group(0).strip()

    # Look for "thá»a thuáº­n" or negotiable
    if any(term in snippet.lower() for term in ['thá»a thuáº­n', 'negotiable', 'cáº¡nh tranh']):
        return "Thá»a thuáº­n"

    return "N/A"


def extract_location_from_snippet(snippet: str) -> str:
    """Extract location from snippet if not provided"""
    vn_locations = ['tp.hcm', 'há»“ chÃ­ minh',
                    'hÃ  ná»™i', 'Ä‘Ã  náºµng', 'cáº§n thÆ¡', 'háº£i phÃ²ng']

    snippet_lower = snippet.lower()
    for location in vn_locations:
        if location in snippet_lower:
            return location.title()

    return "N/A"


def extract_source_from_url(url: str) -> str:
    """Extract source site from URL"""
    if 'topcv.vn' in url:
        return 'topcv'
    elif 'vietnamworks.com' in url:
        return 'vietnamworks'
    elif 'topdev.vn' in url:
        return 'topdev'


def clean_job_title(title: str) -> str:
    """Clean job title from Google results"""
    # Remove site names and extra info
    title = re.sub(r'\s*-\s*(TopCV|VietnamWorks|TopDev|ITviec|CareerBuilder).*',
                   '', title, flags=re.IGNORECASE)
    return title.strip()
